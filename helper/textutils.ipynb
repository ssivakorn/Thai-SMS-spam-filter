{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb2876-258a-4951-90d3-e5ff11c70a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pythainlp\n",
    "\n",
    "print (pythainlp.__version__)\n",
    "\n",
    "from pythainlp import word_tokenize, Tokenizer\n",
    "from pythainlp.corpus.common import thai_words\n",
    "from pythainlp.util import dict_trie\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453ce35-4758-43e8-bf18-7501f1790056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format text\n",
    "# Spaces\n",
    "# Special characters -> space character\n",
    "# Add 1 space character between different languages\n",
    "# Join multiple lines with space character\n",
    "# Multiple spaces -> 1 space character\n",
    "\n",
    "# Others\n",
    "# Link/URL -> <link>\n",
    "# continue digits (129373) -> <digits>\n",
    "# Emojis -> <emoji>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afd2f6d-3d95-4f25-9c5e-75ce96daa894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kLinkPattern =  r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-\\?\\-.]*)*/?)\\b'\n",
    "kLinkPattern = r'\\b((?:https?://)?(?:(?:www\\.)?(?:[\\da-zA-Z\\.-]+)\\.(?:[a-zA-Z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[^ ]+)*/?)\\b'\n",
    "kOTPPattern  = r'(^|[^\\d\\.\\,\\/])+(\\d{4,6})($|[^\\d\\.\\,\\/])+'\n",
    "\n",
    "kLineHosts = [\n",
    "    'lin.ee/',\n",
    "    'line.me/',\n",
    "    'line://',\n",
    "    'liff.line.me/',\n",
    "]\n",
    "\n",
    "kShortLinkHosts = [\n",
    "    'bit.ly/',\n",
    "    'cutt.ly/',\n",
    "    'tinyurl.com/',\n",
    "    'tiny.cc/',\n",
    "    'rb.gy/',\n",
    "    'ow.ly/',\n",
    "    't.co/',\n",
    "]\n",
    "\n",
    "def add_space_before_http(text):\n",
    "    text = text.replace('http://', ' http://')\n",
    "    text = text.replace('https://', ' https://')\n",
    "    return text\n",
    "    \n",
    "def get_all_links(text):\n",
    "    return re.findall(kLinkPattern, text)  \n",
    "\n",
    "def _does_match_hostname(hostnames, link):\n",
    "    link = link.lower().strip()\n",
    "    for host in hostnames:\n",
    "        if (link.startswith(host) or \n",
    "            link.startswith(f'http://{host}') or \\\n",
    "            link.startswith(f'https://{host}')):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "    \n",
    "\n",
    "def has_short_link(links):\n",
    "    # List of public popular short link services (no register required)\n",
    "    \n",
    "    for lnk in links:\n",
    "        if _does_match_hostname(kShortLinkHosts, lnk):\n",
    "            return True\n",
    "    return False\n",
    "              \n",
    "\n",
    "def has_lineapp(links):\n",
    "    for lnk in links:\n",
    "        if _does_match_hostname(kLineHosts, lnk):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_all_OTP(text):\n",
    "    OTPs = list()\n",
    "    for match in re.findall(kOTPPattern, text):\n",
    "        OTPs.append(match[1])\n",
    "    \n",
    "    return OTPs\n",
    "\n",
    "    \n",
    "def remove_all_links(links, text):\n",
    "    \n",
    "    for lnk in links:\n",
    "        if _does_match_hostname(kLineHosts, lnk):\n",
    "            text = text.replace(lnk, '<LINE>')\n",
    "        elif _does_match_hostname(kShortLinkHosts, lnk):\n",
    "            text = text.replace(lnk, '<SHORT-LINK>')\n",
    "    \n",
    "    return re.sub(kLinkPattern, ' <LINK> ', text)\n",
    "\n",
    "def remove_all_otp(text, otp_lst):\n",
    "    for otp in otp_lst:\n",
    "        text = text.replace(otp, ' <OTP> ')\n",
    "    return text\n",
    "\n",
    "\n",
    "kBlankPattern = re.compile(r'\\s+')\n",
    "def remove_blanks(text):\n",
    "    return kBlankPattern.sub(' ', text).strip()\n",
    "\n",
    "\n",
    "def sanitize_text(text, remove_links=True):\n",
    "    \"\"\"Sanitizing text\"\"\"\n",
    "    \n",
    "    text = add_space_before_http(text)\n",
    "    \n",
    "    if remove_links:\n",
    "        links = get_all_links(text) \n",
    "        text  = remove_all_links(links, text)\n",
    "    \n",
    "    text = remove_blanks(text)\n",
    "    text = text.replace('เเ', 'แ') # Fix Thai Grammar\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06c54b-28d5-4a02-9a97-fa2570950760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feaf57d-39a3-40b3-b669-4f960545f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "# Build dictionary (word list)\n",
    "\n",
    "kSpecialChars = '''!\"#$&%()*+,-./:;<=>?@[\\]^_\\'{|}~ '''\n",
    "kSpecialChars = set(kSpecialChars)\n",
    "\n",
    "def is_special_char(word):\n",
    "    if word in kSpecialChars:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_special_chars(words):\n",
    "    new_words = list()\n",
    "    for word in words:\n",
    "        if word == ''.join(list(filter(lambda c: is_special_char(c), word))):\n",
    "            # should split special characters\n",
    "            new_words += word\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "\n",
    "    return new_words\n",
    "\n",
    "kOmittedWords = {'<OTP>'}\n",
    "kCustomWords = {'<LINK>',\n",
    "                '<LINE>',\n",
    "                '<SHORT-LINK>',\n",
    "                '<NAME>'}\n",
    "\n",
    "kOtherWords = set()\n",
    "current_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "custom_words_file = os.path.join(current_dir, \"custom_words.txt\")\n",
    "with open(custom_words_file) as fp:\n",
    "    for line in fp:\n",
    "        kOtherWords.add(line.strip())\n",
    "        \n",
    "# kOtherWords  = {\n",
    "#     'ม.ค.', 'ก.พ.', 'มี.ค.', 'เม.ย.', 'พ.ค.', 'มิ.ย.', 'ก.ค.', 'ส.ค.', 'ก.ย.', 'ต.ค.', 'พ.ย.', 'ธ.ค.',\n",
    "#     'คริปโต', 'บิตคอยต์', 'เครดิด', 'เครดิต', 'วอเลต', 'วอลเล็ต', 'แอป', 'ๆ', '!', 'บ/ช',\n",
    "#     'แอ็กเคานต์', 'แอปพลิเคชัน', 'แชต', 'เช็ก', 'โค้ด', 'คอมเมนต์', 'คอนเทนต์', 'ก๊อปปี้', \n",
    "#     'เคานต์ดาวน์', 'อีเวนต์', 'เกม', 'เกมส์', 'ไฮไลต์', 'แจ็กพอต', 'เมสเซ็นเจอร์', 'มิวสิก', 'เน็ตเวิร์ก' , 'โน้ตบุ๊ก',\n",
    "#     'พาร์ตไทม์', 'พอยต์', 'ปรินต์', 'โพรไฟล์', 'สคริปต์', 'เซนเซอร์', 'เซ็กซ์' , 'ชอปปิง', 'ชอปปิ้ง', 'สมาร์ตโฟน', 'ซับสไครบ์',\n",
    "#     'เต็นท์', 'อัปเดต' , 'อัพเดท', 'อัปเดท', 'อัพเดท', 'เวอร์ชัน', 'เวอร์ชั่น', 'วิดีโอ', 'วีดีโอ', 'วอลล์เปเปอร์', 'เวิร์กชอป', \n",
    "#     'เอกซ์', 'เอ็กซ์', 'เอ๊กซ์',\n",
    "#     'ลิงก์', 'ยูส', 'ยี่กี', 'เทส', 'บาคาร่า', 'ไลน์', 'ลาซาด้า', 'โปรโมชั่น', 'โปร', 'บิต', 'อีสปอร์ต', '฿',\n",
    "#     'พ.ร.บ', 'กธ.'\n",
    "# }\n",
    "\n",
    "kReplaceWords = {\n",
    "    '฿': 'บาท',\n",
    "    'บ': 'บาท',\n",
    "    'น': 'นาฬิกา',\n",
    "    \n",
    "}\n",
    "\n",
    "# Thai words\n",
    "custom_words_list = set(thai_words())\n",
    "custom_words_list.update(kCustomWords.union(kOtherWords))\n",
    "trie = dict_trie(dict_source=custom_words_list)\n",
    "custom_tokenizer = Tokenizer(custom_dict=trie, engine='newmm')\n",
    "\n",
    "def tokenize(text, clean=False):\n",
    "\n",
    "    # tokenize\n",
    "    words = custom_tokenizer.word_tokenize(text)\n",
    "    words = split_special_chars(words)\n",
    "\n",
    "    words = list(filter(lambda w: not is_special_char(w), words))\n",
    "    words = [kReplaceWords[w] if w in kReplaceWords else w for w in words]  \n",
    "\n",
    "    if not clean:\n",
    "        return words\n",
    "\n",
    "    # tokenize + clean\n",
    "    clean_words = list()\n",
    "    for word in words:\n",
    "        if word in kOmittedWords:\n",
    "            continue\n",
    "        if word in kCustomWords:\n",
    "            clean_words.append(word)\n",
    "            continue\n",
    "\n",
    "        # kRegExp = r\"[ก-๙a-zA-Z']+\"\n",
    "        kRegExp = r\"[\\u0E00-\\u0E7Fa-zA-Z' ]+\"\n",
    "        word = ''.join(re.findall(kRegExp, word))\n",
    "        \n",
    "        if (word and\n",
    "            word not in kOmittedWords and \n",
    "            word not in kCustomWords):\n",
    "            word = re.sub(r'[A-Z]', lambda m: m.group(0).lower(), word)\n",
    "        \n",
    "        clean_words += [word] if word else []\n",
    "        \n",
    "    return clean_words\n",
    "\n",
    "\n",
    "class Dictionary:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cvec  = None\n",
    "        self.tfvec = None\n",
    "        self.word_lists = list()\n",
    "        self.bow      = None\n",
    "        self.tfmatrix = None\n",
    "        self.custom_vocab = list()\n",
    "\n",
    "    def add(self, text, clean=True):\n",
    "        words = tokenize(text, clean)\n",
    "        self.word_lists.append(','.join(words))\n",
    "        \n",
    "        for w in words:\n",
    "            self.custom_vocab.append(w)\n",
    "        self.custom_vocab = list(set(self.custom_vocab))\n",
    "        \n",
    "    \n",
    "    def add_completed(self):\n",
    "        self.cvec = CountVectorizer(tokenizer=lambda x:x.split(','), vocabulary=self.custom_vocab)\n",
    "        self.bow = self.cvec.fit_transform(self.word_lists)        \n",
    "\n",
    "        self.tfvec = TfidfVectorizer(tokenizer=lambda x:x.split(','), vocabulary=self.custom_vocab)\n",
    "        self.tfidf = self.tfvec.fit_transform(self.word_lists)\n",
    "    \n",
    "    def get_cvec(self):\n",
    "        return self.cvec\n",
    "    \n",
    "    def get_tfvec(self):\n",
    "        return self.tfvec\n",
    "    \n",
    "    def get_vocabulary(self, vec_type='cvec'):\n",
    "        \n",
    "        if vec_type == 'cvec':\n",
    "            return self.cvec.vocabulary_\n",
    "        elif vec_type == 'tfidf':\n",
    "            return self.tfvec.vocabulary_\n",
    "        else:\n",
    "            raise RuntimeError('No vectorizer type found')\n",
    "        \n",
    "    def get_vocabulary_count(self, vec_type='cvec'):\n",
    "        if vec_type == 'cvec':\n",
    "            return len(self.cvec.vocabulary_)\n",
    "        elif vec_type == 'tfidf':\n",
    "            return len(self.tfvec.vocabulary_)\n",
    "        else:\n",
    "            raise RuntimeError('No vectorizer type found')  \n",
    "    \n",
    "    def get_bag_of_words(self):\n",
    "        return self.bow.toarray()\n",
    "    \n",
    "    def get_tfidf(self):\n",
    "        return self.tfidf.toarray()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec55d3f-2126-49fb-9b19-3800e0e03f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_seq(words, word_index, maxlen=50, ignore_err=False):\n",
    "    \n",
    "    sequence = list()\n",
    "    for w in words:\n",
    "        if w not in word_index:\n",
    "            error_msg = f'Found word not in word_index: word={w}'\n",
    "            if ignore_err:\n",
    "                logger.warning(error_msg)\n",
    "                continue\n",
    "            else:\n",
    "                raise RuntimeError(error_msg)\n",
    "            \n",
    "        sequence.append(word_index[w])\n",
    "        \n",
    "    if len(sequence) < maxlen:\n",
    "        sequence += [0] * (maxlen - len(sequence))\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "def padded_seq_from_text(text, word_index, maxlen=50, ignore_err=False):\n",
    "    \n",
    "    words = tokenize(text, clean=True)\n",
    "    return padded_seq(words, word_index, maxlen, ignore_err=ignore_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab7ce44-6efc-4310-b8c6-8ded66bf2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_array(words, word_index, ignore_err=False):\n",
    "    bow_array = np.zeros(len(word_index))\n",
    "    for w in words:\n",
    "        if w not in word_index:\n",
    "            error_msg = f'Found word not in word_index: word={w}'\n",
    "            if ignore_err:\n",
    "                logger.warning(error_msg)\n",
    "                continue\n",
    "                \n",
    "        bow_array[word_index[w]] += 1\n",
    "    return bow_array\n",
    "    \n",
    "    \n",
    "def bow_array_from_text(text, word_index, ignore_err=False):\n",
    "    words = tokenize(text, clean=True)\n",
    "    return bow_array(words, word_index, ignore_err=ignore_err)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb0b9a-2931-4de2-a8ea-c5ef87e5eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_array(words, word_index, idf_values, ignore_err=False):\n",
    "    tfidf_array = np.zeros(len(word_index))\n",
    "    for w in words:\n",
    "        if w not in word_index:\n",
    "            error_msg = f'Found word not in word_index: word={w}'\n",
    "            if ignore_err:\n",
    "                logger.warning(error_msg)\n",
    "                continue\n",
    "                \n",
    "        # tfidf_array[word_index[w]] += 1\n",
    "        tf  = words.count(w) / len(words) # term frequency\n",
    "        idf = idf_values[word_index[w]]   # inverse document frequency\n",
    "        tfidf_array[word_index[w]] = tf * idf\n",
    "        \n",
    "    return tfidf_array\n",
    "    \n",
    "    \n",
    "def tfidf_array_from_text(text, word_index, idf_values, ignore_err=False):\n",
    "    words = tokenize(text, clean=True)\n",
    "    return tfidf_array(words, word_index, idf_values, ignore_err=ignore_err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
